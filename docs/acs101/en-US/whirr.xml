<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
<!ENTITY % BOOK_ENTITIES SYSTEM "cloudstack.ent">
%BOOK_ENTITIES;
]>

<!-- Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at
 
   http://www.apache.org/licenses/LICENSE-2.0
 
 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
-->

<section id="whirr">
    <title>Apache Whirr</title>
    <para><ulink url="http://whirr.apache.org">Apache Whirr</ulink> is a set of libraries to run cloud services, internally it uses <ulink url="http://jclouds.incubator.apache.org">jclouds</ulink> that we introduced earlier via the jclouds-cli interface to &PRODUCT;, it is java based and of interest to provision clusters of virtual machines on cloud providers. Historically it started as a set of scripts to deploy <ulink url="http://hadoop.apache.org">Hadoop</ulink> clusters on Amazon EC2. We introduce Whirr has a potential &PRODUCT; tool to provision Hadoop cluster on &PRODUCT; based clouds.</para>

    <section id="whirr-install">
    <title>Installing Apache Whirr</title>
        <para>
            To install Whirr you can follow the <ulink url="http://whirr.apache.org/docs/0.8.1/quick-start-guide.html">Quick Start Guide</ulink>, download a tarball or clone the git repository. In the spirit of this document we clone the repo:
        </para>
        <programlisting>
git clone git://git.apache.org/whirr.git
        </programlisting>
        <para>
            And build the source with maven that we now know and love...:
        </para>
        <programlisting>
mvn install        
        </programlisting>
        <para>
            The whirr binary will be available in the <emphasis>bin</emphasis> directory that we can add to our path
        </para>	
        <programlisting>
export PATH=$PATH:/Users/sebgoa/Documents/whirr/bin
        </programlisting>
        <para>
            If all went well you should now be able to get the usage of <emphasis>whirr</emphasis>:
        </para>
        <programlisting>
$ whirr --help
Unrecognized command '--help'

Usage: whirr COMMAND [ARGS]
where COMMAND may be one of:

  launch-cluster  Launch a new cluster running a service.
  start-services  Start the cluster services.
   stop-services  Stop the cluster services.
restart-services  Restart the cluster services.
 destroy-cluster  Terminate and cleanup resources for a running cluster.
destroy-instance  Terminate and cleanup resources for a single instance.
    list-cluster  List the nodes in a cluster.
  list-providers  Show a list of the supported providers
      run-script  Run a script on a specific instance or a group of instances matching a role name
         version  Print the version number and exit.
            help  Show help about an action

Available roles for instances:
  cassandra
  elasticsearch
  ganglia-metad
  ganglia-monitor
  hadoop-datanode
  hadoop-jobtracker
  hadoop-namenode
  hadoop-tasktracker
  hama-groomserver
  hama-master
  hbase-avroserver
  hbase-master
  hbase-regionserver
  hbase-restserver
  hbase-thriftserver
  kerberosclient
  kerberosserver
  mahout-client
  mapreduce-historyserver
  noop
  pig-client
  puppet-install
  solr
  yarn-nodemanager
  yarn-resourcemanager
  zookeeper
        </programlisting>
        <para>
            From the look of the usage you clearly see that <emphasis>whirr</emphasis> is about more than just <emphasis>hadoop</emphasis> and that it can be used to configure <emphasis>elasticsearch</emphasis> clusters, <emphasis>cassandra</emphasis> databases as well as the entire <emphasis>hadoop</emphasis> ecosystem with <emphasis>mahout</emphasis>, <emphasis>pig</emphasis>, <emphasis>hbase</emphasis>, <emphasis>hama</emphasis>, <emphasis>mapreduce</emphasis> and <emphasis>yarn</emphasis>.
        </para>
    </section>

    <section id="whirr-use">
    <title>Using Apache Whirr</title>
        <para>
            To get started with Whirr you need to setup the credentials and endpoint of your &PRODUCT; based cloud that you will be using. Edit the <emphasis>~/.whirr/credentials</emphasis> file to include a PROVIDER, IDENTITY, CREDENTIAL and ENDPOINT. The PROVIDER needs to be set to <emphasis>cloudstack</emphasis>, the IDENTITY is your API key, the CREDENTIAL is your secret key and the ENDPPOINT is the endpoint url. For instance:
        </para>
        <para>
        <programlisting>
PROVIDER=cloudstack
IDENTITY=mnH5EbKc4534592347523486724389673248AZW4kYV5gdsfgdfsgdsfg87sdfohrjktn5Q
CREDENTIAL=Hv97W58iby5PWL1ylC4oJls46456435634564537sdfgdfhrteydfg87sdf89gysdfjhlicg
ENDPOINT=https://api.exoscale.ch/compute
        </programlisting>
        </para>
        <para>
            With the credentials and endpoint defined you can create a <emphasis>properties</emphasis> file that describes the cluster you want to launch on your cloud. The file contains information such as the cluster name, the number of instances and their type, the distribution of hadoop you want to use, the service offering id and the template id of the instances. It also defines the ssh keys to be used for accessing the virtual machines. In the case of a cloud that uses security groups, you may also need to specify it. A tricky point is the handling of DNS name resolution. You might have to use the <emphasis>whirr.store-cluster-in-etc-hosts</emphasis> key to bypass any DNS issues. For a full description of the whirr property keys, see the <ulink url="http://whirr.apache.org/docs/0.8.1/configuration-guide.html">documentation</ulink>.
        </para>
        <para>
        <programlisting>
$ more whirr.properties 

#
# Setup an Apache Hadoop Cluster
# 

# Change the cluster name here
whirr.cluster-name=hadoop

whirr.store-cluster-in-etc-hosts=true

whirr.use-cloudstack-security-group=true

# Change the name of cluster admin user
whirr.cluster-user=${sys:user.name}

# Change the number of machines in the cluster here
whirr.instance-templates=1 hadoop-namenode+hadoop-jobtracker,3 hadoop-datanode+hadoop-tasktracker

# Uncomment out the following two lines to run CDH
whirr.env.repo=cdh4
whirr.hadoop.install-function=install_cdh_hadoop
whirr.hadoop.configure-function=configure_cdh_hadoop

whirr.hardware-id=b6cd1ff5-3a2f-4e9d-a4d1-8988c1191fe8

whirr.private-key-file=/path/to/ssh/key/
whirr.public-key-file=/path/to/ssh/public/key/

whirr.provider=cloudstack
whirr.endpoint=https://the/endpoint/url
whirr.image-id=1d16c78d-268f-47d0-be0c-b80d31e765d2
        </programlisting>
        </para>
        <warning>
            <para>
                The example shown above is specific to a production <ulink url="http://exoscale.ch">Cloud</ulink> setup as a basic zone. This cloud uses security groups for isolation between instances. The proper rules had to be setup by hand. Also note the use of <emphasis>whirr.store-cluster-in-etc-hosts</emphasis>. If set to true whirr will edit the <emphasis>/etc/hosts</emphasis> file of the nodes and enter the IP adresses. This is handy in the case where DNS resolution is problematic.
            </para>
        </warning>
        <note>
            <para>
                To use the Cloudera Hadoop distribution (CDH) like in the example above, you will need to copy the <emphasis>services/cdh/src/main/resources/functions</emphasis> directory to the root of your Whirr source. In this directory you will find the bash scripts used to bootstrap the instances. It may be handy to edit those scripts.
            </para>
        </note>
        <para>
            You are now ready to launch an hadoop cluster:
        </para>
        <para>
        <programlisting>
<![CDATA[
$ whirr launch-cluster --config hadoop.properties 
Running on provider cloudstack using identity mnH5EbKcKeJd456456345634563456345654634563456345
Bootstrapping cluster
Configuring template for bootstrap-hadoop-datanode_hadoop-tasktracker
Configuring template for bootstrap-hadoop-namenode_hadoop-jobtracker
Starting 3 node(s) with roles [hadoop-datanode, hadoop-tasktracker]
Starting 1 node(s) with roles [hadoop-namenode, hadoop-jobtracker]
>> running InitScript{INSTANCE_NAME=bootstrap-hadoop-datanode_hadoop-tasktracker} on node(b9457a87-5890-4b6f-9cf3-1ebd1581f725)
>> running InitScript{INSTANCE_NAME=bootstrap-hadoop-datanode_hadoop-tasktracker} on node(9d5c46f8-003d-4368-aabf-9402af7f8321)
>> running InitScript{INSTANCE_NAME=bootstrap-hadoop-datanode_hadoop-tasktracker} on node(6727950e-ea43-488d-8d5a-6f3ef3018b0f)
>> running InitScript{INSTANCE_NAME=bootstrap-hadoop-namenode_hadoop-jobtracker} on node(6a643851-2034-4e82-b735-2de3f125c437)
<< success executing InitScript{INSTANCE_NAME=bootstrap-hadoop-datanode_hadoop-tasktracker} on node(b9457a87-5890-4b6f-9cf3-1ebd1581f725): {output=This function does nothing. It just needs to exist so Statements.call("retry_helpers") doesn't call something which doesn't exist
Get:1 http://security.ubuntu.com precise-security Release.gpg [198 B]
Get:2 http://security.ubuntu.com precise-security Release [49.6 kB]
Hit http://ch.archive.ubuntu.com precise Release.gpg
Get:3 http://ch.archive.ubuntu.com precise-updates Release.gpg [198 B]
Get:4 http://ch.archive.ubuntu.com precise-backports Release.gpg [198 B]
Hit http://ch.archive.ubuntu.com precise Release
..../snip/.....
You can log into instances using the following ssh commands:
[hadoop-datanode+hadoop-tasktracker]: ssh -i /Users/sebastiengoasguen/.ssh/id_rsa -o "UserKnownHostsFile /dev/null" -o StrictHostKeyChecking=no sebastiengoasguen@185.xx.yy.zz
[hadoop-datanode+hadoop-tasktracker]: ssh -i /Users/sebastiengoasguen/.ssh/id_rsa -o "UserKnownHostsFile /dev/null" -o StrictHostKeyChecking=no sebastiengoasguen@185.zz.zz.rr
[hadoop-datanode+hadoop-tasktracker]: ssh -i /Users/sebastiengoasguen/.ssh/id_rsa -o "UserKnownHostsFile /dev/null" -o StrictHostKeyChecking=no sebastiengoasguen@185.tt.yy.uu
[hadoop-namenode+hadoop-jobtracker]: ssh -i /Users/sebastiengoasguen/.ssh/id_rsa -o "UserKnownHostsFile /dev/null" -o StrictHostKeyChecking=no sebastiengoasguen@185.ii.oo.pp
To destroy cluster, run 'whirr destroy-cluster' with the same options used to launch it.
]]>
        </programlisting>
        </para>
        <para>
            After the boostrapping process finishes, you should be able to login to your instances and use <emphasis>hadoop</emphasis> or if you are running a proxy on your machine, you will be able to access your hadoop cluster locally. Testing of Whirr for &PRODUCT; is still under <ulink url="https://issues.apache.org/jira/browse/WHIRR-725">investigation</ulink> and the subject of a Google Summer of Code 2013 project. We currently identified issues with the use of security groups. Moreover this was tested on a basic zone. Complete testing on an advanced zone is future work.
        </para>
    </section>

    <section id="using-map-reduce">
    <title>Running Map-Reduce jobs on Hadoop</title>
        <para>
        Whirr gives you the ssh command to connect to the instances of your hadoop cluster, login to the namenode and browse the hadoop file system that was created:
        </para>
        <programlisting>
$ hadoop fs -ls /
Found 5 items
drwxrwxrwx   - hdfs supergroup          0 2013-06-21 20:11 /hadoop
drwxrwxrwx   - hdfs supergroup          0 2013-06-21 20:10 /hbase
drwxrwxrwx   - hdfs supergroup          0 2013-06-21 20:10 /mnt
drwxrwxrwx   - hdfs supergroup          0 2013-06-21 20:11 /tmp
drwxrwxrwx   - hdfs supergroup          0 2013-06-21 20:11 /user
        </programlisting>
		<para>Create a directory to put your input data</para>
        <programlisting>
$ hadoop fs -mkdir input
$ hadoop fs -ls /user/sebastiengoasguen
Found 1 items
drwxr-xr-x   - sebastiengoasguen supergroup          0 2013-06-21 20:15 /user/sebastiengoasguen/input
        </programlisting>
        <para>Create a test input file and put in the hadoop file system:</para>
		<programlisting>
$ cat foobar 
this is a test to count the words
$ hadoop fs -put ./foobar input
$ hadoop fs -ls /user/sebastiengoasguen/input
Found 1 items
-rw-r--r--   3 sebastiengoasguen supergroup         34 2013-06-21 20:17 /user/sebastiengoasguen/input/foobar
        </programlisting>
        <para>Define the map-reduce environment. Note that this default Cloudera distribution installation uses MRv1. To use Yarn one would have to edit the hadoop.properties file.
        </para>
        <programlisting>
$ export HADOOP_MAPRED_HOME=/usr/lib/hadoop-0.20-mapreduce
        </programlisting>
        <para>Start the map-reduce job:</para>
        <programlisting>
<![CDATA[
			$ hadoop jar $HADOOP_MAPRED_HOME/hadoop-examples.jar wordcount input output
			13/06/21 20:19:59 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
			13/06/21 20:20:00 INFO input.FileInputFormat: Total input paths to process : 1
			13/06/21 20:20:00 INFO mapred.JobClient: Running job: job_201306212011_0001
			13/06/21 20:20:01 INFO mapred.JobClient:  map 0% reduce 0%
			13/06/21 20:20:11 INFO mapred.JobClient:  map 100% reduce 0%
			13/06/21 20:20:17 INFO mapred.JobClient:  map 100% reduce 33%
			13/06/21 20:20:18 INFO mapred.JobClient:  map 100% reduce 100%
			13/06/21 20:20:21 INFO mapred.JobClient: Job complete: job_201306212011_0001
			13/06/21 20:20:22 INFO mapred.JobClient: Counters: 32
			13/06/21 20:20:22 INFO mapred.JobClient:   File System Counters
			13/06/21 20:20:22 INFO mapred.JobClient:     FILE: Number of bytes read=133
			13/06/21 20:20:22 INFO mapred.JobClient:     FILE: Number of bytes written=766347
			13/06/21 20:20:22 INFO mapred.JobClient:     FILE: Number of read operations=0
			13/06/21 20:20:22 INFO mapred.JobClient:     FILE: Number of large read operations=0
			13/06/21 20:20:22 INFO mapred.JobClient:     FILE: Number of write operations=0
			13/06/21 20:20:22 INFO mapred.JobClient:     HDFS: Number of bytes read=157
			13/06/21 20:20:22 INFO mapred.JobClient:     HDFS: Number of bytes written=50
			13/06/21 20:20:22 INFO mapred.JobClient:     HDFS: Number of read operations=2
			13/06/21 20:20:22 INFO mapred.JobClient:     HDFS: Number of large read operations=0
			13/06/21 20:20:22 INFO mapred.JobClient:     HDFS: Number of write operations=3
			13/06/21 20:20:22 INFO mapred.JobClient:   Job Counters 
			13/06/21 20:20:22 INFO mapred.JobClient:     Launched map tasks=1
			13/06/21 20:20:22 INFO mapred.JobClient:     Launched reduce tasks=3
			13/06/21 20:20:22 INFO mapred.JobClient:     Data-local map tasks=1
			13/06/21 20:20:22 INFO mapred.JobClient:     Total time spent by all maps in occupied slots (ms)=10956
			13/06/21 20:20:22 INFO mapred.JobClient:     Total time spent by all reduces in occupied slots (ms)=15446
			13/06/21 20:20:22 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
			13/06/21 20:20:22 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
			13/06/21 20:20:22 INFO mapred.JobClient:   Map-Reduce Framework
			13/06/21 20:20:22 INFO mapred.JobClient:     Map input records=1
			13/06/21 20:20:22 INFO mapred.JobClient:     Map output records=8
			13/06/21 20:20:22 INFO mapred.JobClient:     Map output bytes=66
			13/06/21 20:20:22 INFO mapred.JobClient:     Input split bytes=123
			13/06/21 20:20:22 INFO mapred.JobClient:     Combine input records=8
			13/06/21 20:20:22 INFO mapred.JobClient:     Combine output records=8
			13/06/21 20:20:22 INFO mapred.JobClient:     Reduce input groups=8
			13/06/21 20:20:22 INFO mapred.JobClient:     Reduce shuffle bytes=109
			13/06/21 20:20:22 INFO mapred.JobClient:     Reduce input records=8
			13/06/21 20:20:22 INFO mapred.JobClient:     Reduce output records=8
			13/06/21 20:20:22 INFO mapred.JobClient:     Spilled Records=16
			13/06/21 20:20:22 INFO mapred.JobClient:     CPU time spent (ms)=1880
			13/06/21 20:20:22 INFO mapred.JobClient:     Physical memory (bytes) snapshot=469413888
			13/06/21 20:20:22 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=5744541696
			13/06/21 20:20:22 INFO mapred.JobClient:     Total committed heap usage (bytes)=207687680
]]>
        </programlisting>
        <para> And you can finally check the output:</para>
        <programlisting>
$ hadoop fs -cat output/part-* | head
this	1
to		1
the		1
a		1
count	1
is		1
test	1
words	1
        </programlisting>            
    </section>

</section>
